{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/research/jesnk_packages/gym_robotics/envs/fetch/reach.py\n"
     ]
    }
   ],
   "source": [
    "# train with SAC, stable baseline3\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import SAC, PPO\n",
    "from stable_baselines3.sac import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "import wandb\n",
    "# pip install gym-robotics\n",
    "import matplotlib.pyplot as plt\n",
    "from gym_robotics.envs.fetch.reach import MujocoPyFetchReachEnv\n",
    "from gym.wrappers import TimeLimit\n",
    "from jesnk_utils.rgb_to_video import RGB2VIDEO\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint_path = \"./checkpoint/FetchReach-SAC-dense-20230701_154341.zip\"\n",
    "#load_checkpoint_path = \"./checkpoint/PPO-FetchReach-dense-20230621_004058.zip\"\n",
    "model_name = load_checkpoint_path.split(\"/\")[-1].split(\".\")[0]\n",
    "rollout_path = f\"./checkpoint_rollout/{model_name}/\"\n",
    "model = SAC.load(load_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/research/jesnk_packages/gym_robotics/envs/robot_env.py:330: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "/research/jesnk_packages/gym_robotics/envs/fetch_env.py:256: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ret = np.array(ret)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success rate: 0.856\n"
     ]
    }
   ],
   "source": [
    "env = MujocoPyFetchReachEnv(reward_type='dense')\n",
    "env.render_mode = 'rgb_array'\n",
    "#env = Monitor(env_eval, log_dir)\n",
    "#env = TimeLimit(env, max_episode_steps=100)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "#env.render_mode = 'rgb_array'\n",
    "\n",
    "episode_step = 0\n",
    "current_episode = 0\n",
    "max_episode_num = 1000\n",
    "cumulative_reward = 0\n",
    "frames = []\n",
    "obs = env.reset()\n",
    "success = []\n",
    "episodes = []\n",
    "\n",
    "def init_episode_dict() :\n",
    "    episode = {}\n",
    "    # set keys of episode 'observations', 'next_observations', 'actions', 'rewards', 'terminals'\n",
    "    for key in ['observations', 'next_observations', 'actions', 'rewards', 'terminals']:\n",
    "        episode[key] = []\n",
    "    return episode\n",
    "\n",
    "def convert_ordered_dict_to_array(ordered_dict):\n",
    "    ret = np.array([])\n",
    "    for key in ordered_dict.keys():\n",
    "        ret = np.append(ret, ordered_dict[key])\n",
    "    return np.array(ret.reshape(-1))\n",
    "\n",
    "episode = init_episode_dict()\n",
    "\n",
    "while current_episode < max_episode_num:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    \n",
    "    postprocessed_obs = convert_ordered_dict_to_array(obs)\n",
    "    episode['observations'].append(postprocessed_obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    cumulative_reward += rewards[0]\n",
    "    postprocessed_obs = convert_ordered_dict_to_array(obs)\n",
    "    episode['next_observations'].append(postprocessed_obs)\n",
    "    episode['actions'].append(action)\n",
    "    episode['rewards'].append(rewards[0])\n",
    "    episode['terminals'].append(dones[0])\n",
    "    \n",
    "    episode_step += 1\n",
    "    if dones[0]:\n",
    "        obs = env.reset()\n",
    "        success.append(info[0]['is_success'])\n",
    "        episode_step = 0\n",
    "        cumulative_reward = 0\n",
    "        current_episode += 1\n",
    "        episode['observations'] = np.array(episode['observations'])\n",
    "        episode['next_observations'] = np.array(episode['next_observations'])\n",
    "        episode['actions'] = np.array(episode['actions'])\n",
    "        episode['terminals'] = np.array(episode['terminals'])\n",
    "        episode['rewards'] = np.array(episode['rewards'])\n",
    "        episodes.append(episode)\n",
    "        episode = init_episode_dict()\n",
    "\n",
    "success_rate = sum(success)/len(success)\n",
    "print(f'success rate: {success_rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'observation': array([ 1.34185463e+00,  7.49100507e-01,  5.34707334e-01,  2.00241622e-04,\n",
      "        6.92290737e-05, -3.14178004e-06, -1.64992873e-09,  5.11115602e-06,\n",
      "        4.76895682e-06, -2.31822942e-06]), 'achieved_goal': array([1.34185463, 0.74910051, 0.53470733]), 'desired_goal': array([1.35849046, 0.70055753, 0.63110757])}, {})\n",
      "{'observation': array([ 1.34185463e+00,  7.49100507e-01,  5.34707334e-01,  2.00241622e-04,\n",
      "        6.92290737e-05, -3.14178004e-06, -1.64992873e-09,  5.11115602e-06,\n",
      "        4.76895682e-06, -2.31822942e-06]), 'achieved_goal': array([1.34185463, 0.74910051, 0.53470733]), 'desired_goal': array([1.35849046, 0.70055753, 0.63110757])}\n",
      "{'observation': array([ 1.34185463e+00,  7.49100507e-01,  5.34707334e-01,  2.00241622e-04,\n",
      "        6.92290737e-05, -3.14178004e-06, -1.64992873e-09,  5.11115602e-06,\n",
      "        4.76895682e-06, -2.31822942e-06]), 'achieved_goal': array([1.34185463, 0.74910051, 0.53470733]), 'desired_goal': array([1.35849046, 0.70055753, 0.63110757])}\n"
     ]
    }
   ],
   "source": [
    "from jesnk_utils.utils import convert_ordered_dict_to_array, convert_to_serialized_array\n",
    "import torch\n",
    "from gym_robotics.envs.fetch.reach import MujocoPyFetchReachEnv\n",
    "\n",
    "env = MujocoPyFetchReachEnv(reward_type='dense')\n",
    "state = convert_to_serialized_array(env.reset())\n",
    "\n",
    "state_dim = state.shape[0]\n",
    "device = 'cuda'\n",
    "cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20230717_043210'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "# get current time from online server\n",
    "cur_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "cur_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./offline_data/FetchReach-SAC-dense-20230701_154341.pkl\n"
     ]
    }
   ],
   "source": [
    "# save data as pickle\n",
    "import datetime\n",
    "import pickle\n",
    "time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "path = './offline_data/{}'.format(model_name+'.pkl')\n",
    "print(path)\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(episodes, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('achieved_goal': Box(-inf, inf, (3,), float64), 'desired_goal': Box(-inf, inf, (3,), float64), 'observation': Box(-inf, inf, (10,), float64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
